{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CzjTelUVyFkv",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Del3hgyTNEk5"
      },
      "source": [
        "**Sebelum Lanjut!,   jalankan \"Librari Wrapper-Forward\" di bagian paling bawah**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mU9Zy447yFk3",
        "outputId": "9d99a8b0-7efe-473c-ab7a-0b25a26ceac2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        }
      },
      "source": [
        "#data_cleanU1.xlsx\n",
        "data = pd.read_excel(\"https://github.com/lufias69/Spam_Detection/blob/master/Filter%20chi%20square/komentar/data_cleanU1.xlsx?raw=true\")\n",
        "# data = data[:200]\n",
        "komentar = data.komentar.tolist()\n",
        "label = data.label.tolist()\n",
        "\n",
        "y = np.array(label)\n",
        "print(len(y))\n",
        "\n",
        "data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Column1</th>\n",
              "      <th>label</th>\n",
              "      <th>komentar</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>195</td>\n",
              "      <td>spam</td>\n",
              "      <td>ga nyesel deh aku order sana</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>480</td>\n",
              "      <td>non spam</td>\n",
              "      <td>wakakak tolol tulang tulang</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>539</td>\n",
              "      <td>non spam</td>\n",
              "      <td>sepatu keren kamu keren cara instan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>564</td>\n",
              "      <td>spam</td>\n",
              "      <td>via q udh blja shoppe barang bagus kecewa mantap</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>596</td>\n",
              "      <td>non spam</td>\n",
              "      <td>sir what if use again kamisenglish that s just...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195</th>\n",
              "      <td>2612</td>\n",
              "      <td>non spam</td>\n",
              "      <td>moga sehat selalu sukses karirnya mba ayu maka...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>196</th>\n",
              "      <td>2674</td>\n",
              "      <td>non spam</td>\n",
              "      <td>wah wah kalo gitu arti tv nya dong nontonin mb...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197</th>\n",
              "      <td>2713</td>\n",
              "      <td>non spam</td>\n",
              "      <td>klu goblok tu jngn kbangetan mau pilih hak tpi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198</th>\n",
              "      <td>2750</td>\n",
              "      <td>non spam</td>\n",
              "      <td>pegang ranah hukum bila masalah dr gk sama sek...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199</th>\n",
              "      <td>2756</td>\n",
              "      <td>non spam</td>\n",
              "      <td>ye dasar wibu gininih kalo cacing bosen perut ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>200 rows Ã— 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Column1     label                                           komentar\n",
              "0        195      spam                       ga nyesel deh aku order sana\n",
              "1        480  non spam                        wakakak tolol tulang tulang\n",
              "2        539  non spam                sepatu keren kamu keren cara instan\n",
              "3        564      spam   via q udh blja shoppe barang bagus kecewa mantap\n",
              "4        596  non spam  sir what if use again kamisenglish that s just...\n",
              "..       ...       ...                                                ...\n",
              "195     2612  non spam  moga sehat selalu sukses karirnya mba ayu maka...\n",
              "196     2674  non spam  wah wah kalo gitu arti tv nya dong nontonin mb...\n",
              "197     2713  non spam  klu goblok tu jngn kbangetan mau pilih hak tpi...\n",
              "198     2750  non spam  pegang ranah hukum bila masalah dr gk sama sek...\n",
              "199     2756  non spam  ye dasar wibu gininih kalo cacing bosen perut ...\n",
              "\n",
              "[200 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoPekKK7cPVf",
        "colab_type": "text"
      },
      "source": [
        "**seleksi fitur menggunakan filter**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHKN9dZzcKNB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "sf = cs.ChiSquare(alpha = 0.1)\n",
        "#inisialisasi\n",
        "sf.find_best_features(komentar, label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "t9c7riZ-yFk_",
        "outputId": "bbb415dd-6b1f-43c1-bc4b-1228503e89f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "#ini lama bangets\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "hasil = wrapper(model=MultinomialNB(), weighting=TfidfVectorizer, corpus=komentar, y=y,n_splits=5, target=0.97)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "$.^^.\n",
            "1| 0.885\n",
            "2| 0.885\n",
            "3| 0.885\n",
            "4| 0.885\n",
            "5| 0.885\n",
            "6| 0.885\n",
            "7| 0.885\n",
            "8| 0.885\n",
            "9| 0.885\n",
            "10| 0.885\n",
            "11| 0.885\n",
            "12| 0.885\n",
            "13| 0.885\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sNnA5SntyFlE",
        "colab": {}
      },
      "source": [
        "best_fitur = hasil[1]\n",
        "\n",
        "for i in best_fitur:\n",
        "    print(i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jNoLjYjbyFlJ",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jmNI1P8syFlO",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "r8Rj-XSPyFlT",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8frkFsAUyFlf",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6YEp-P_nJ1ow"
      },
      "source": [
        "**Librari Wrapper-Forward**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAvinpiKx0F3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "def convert_bin(data, key=False):\n",
        "    new_data = list()\n",
        "    if (type(data)==list or type(data)==str) and key==False:\n",
        "        if type(data)==str:\n",
        "            for d in data:\n",
        "                new_data.append(int(d))\n",
        "        else:\n",
        "            for d in data:\n",
        "                new_data.append(int(d))\n",
        "        return new_data\n",
        "    else:\n",
        "        for d in data:\n",
        "            new_data.append(str(d))\n",
        "        return \"\".join(new_data)\n",
        "            \n",
        "def join_bin(data1, data2):\n",
        "    new_data = list()\n",
        "    for a,b in zip(data1, data2):\n",
        "        if a==b:\n",
        "            new_data.append(a)\n",
        "        elif a!=b:\n",
        "            if b==1:\n",
        "                new_data.append(b)\n",
        "            elif a==1:\n",
        "                new_data.append(a)\n",
        "            else:\n",
        "                new_data.append(0)\n",
        "    return new_data\n",
        "\n",
        "def gen(x, urutan):\n",
        "    first_bin = [0 for i in range(0,x)]\n",
        "    first_bin[urutan]=1\n",
        "    return first_bin\n",
        "\n",
        "def generate_bins(n):\n",
        "    bin_all = list()\n",
        "    for i in range(0,n):\n",
        "        bin_gen = gen(n,i)\n",
        "        bin_all.append(bin_gen)\n",
        "    return bin_all\n",
        "\n",
        "def convert_to_index(data):\n",
        "    new_data = list()\n",
        "    for idx, i in enumerate(data):\n",
        "        if i!=0:\n",
        "            new_data.append(idx)\n",
        "    return new_data\n",
        "\n",
        "\n",
        "\n",
        "def wrapper(model=None, weighting=TfidfVectorizer(), vocabs, corpus=None, y=None,n_splits=5, target=0.97):\n",
        "    \n",
        "    y=np.array(y)\n",
        "    vectorizer = weighting(vocabulary = vocabs)\n",
        "    X = vectorizer.fit_transform(corpus)\n",
        "    fitur = np.array(vectorizer.get_feature_names())\n",
        "    panjang_fitur = len(fitur)\n",
        "    \n",
        "    binari_pertama = generate_bins(panjang_fitur)\n",
        "    scor_binari_pertama = list()\n",
        "    print(\"$\",end=\".\")\n",
        "    for b in binari_pertama:\n",
        "        idx_bin = convert_to_index(b)\n",
        "        # print(fitur[idx_bin],end=\"\")\n",
        "        vectorizer = weighting(vocabulary=fitur[idx_bin])\n",
        "        X = vectorizer.fit_transform(corpus)\n",
        "        \n",
        "        skf = StratifiedKFold(n_splits=n_splits)\n",
        "        skor_list = list()\n",
        "        \n",
        "        for train_index, test_index in skf.split(X, y):\n",
        "            X_train, X_test = X[train_index], X[test_index]\n",
        "            y_train, y_test = y[train_index], y[test_index]\n",
        "            \n",
        "            model.fit(X_train, y_train)\n",
        "            skor_list.append(model.score(X_test,y_test))\n",
        "            \n",
        "        scor_binari_pertama.append(sum(skor_list)/len(skor_list))\n",
        "    print(\"^^\",end=\".\") \n",
        "    print(\"\")\n",
        "    max_index = scor_binari_pertama.index(max(scor_binari_pertama))\n",
        "    pilihan_pertama_bin = binari_pertama[max_index]\n",
        "    print(sum(pilihan_pertama_bin), end=\"| \")\n",
        "    print(max(scor_binari_pertama))\n",
        "    \n",
        "    all_bin = list()\n",
        "    all_skor = list()\n",
        "\n",
        "    best_fitur = list()\n",
        "    best_skor = list()\n",
        "    \n",
        "    for _ in range(panjang_fitur-1):\n",
        "        # print(_, end=\".\")\n",
        "        new_bin_ = list()\n",
        "        \n",
        "        for idx, i in enumerate(binari_pertama):\n",
        "          if convert_bin(i, key=True) != convert_bin(pilihan_pertama_bin, key=True):\n",
        "            # if idx !=pilihan_pertama_bin:\n",
        "            new_bin_.append(join_bin(pilihan_pertama_bin, i))\n",
        "                \n",
        "        # print(sum(new_bin_[0]))\n",
        "        \n",
        "        binari_pertama = list(new_bin_)\n",
        "        scor_binari_pertama = list()\n",
        "        for b in new_bin_:\n",
        "            idx_bin = convert_to_index(b)\n",
        "            vectorizer = weighting(vocabulary=fitur[idx_bin])\n",
        "            X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "            skf = StratifiedKFold(n_splits=n_splits)\n",
        "            skor_list = list()\n",
        "            for train_index, test_index in skf.split(X, y):\n",
        "                X_train, X_test = X[train_index], X[test_index]\n",
        "                y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "                model.fit(X_train, y_train)\n",
        "                skor_list.append(model.score(X_test,y_test))\n",
        "\n",
        "            scor_binari_pertama.append(sum(skor_list)/len(skor_list))\n",
        "        #\n",
        "        max_index = scor_binari_pertama.index(max(scor_binari_pertama))\n",
        "        pilihan_pertama_bin = new_bin_[max_index]\n",
        "        print(sum(pilihan_pertama_bin), end=\"| \")\n",
        "\n",
        "        print(max(scor_binari_pertama))\n",
        "\n",
        "        best_fitur.append(fitur[convert_to_index(pilihan_pertama_bin)])\n",
        "        best_skor.append(max(scor_binari_pertama))\n",
        "        \n",
        "        # all_bin += list(new_bin_)\n",
        "        # all_skor += list(scor_binari_pertama)\n",
        "        \n",
        "        if max(scor_binari_pertama) >= target:\n",
        "            return (max(scor_binari_pertama), fitur[convert_to_index(pilihan_pertama_bin)])\n",
        "        elif sum(pilihan_pertama_bin)>= panjang_fitur-1:\n",
        "          return (max(scor_binari_pertama), fitur[convert_to_index(pilihan_pertama_bin)])\n",
        "        \n",
        "    max_index = best_skor.index(max(best_skor))\n",
        "    # pilihan_pertama_bin = new_bin_[max_index]\n",
        "    best_of_dbest_fitur = best_fitur[max_index]\n",
        "    return (max(best_skor), best_of_dbest_fitur)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhB_jcwoYUou",
        "colab_type": "code",
        "outputId": "f336b0a1-9a30-4c3d-c629-d8d10ba79b4f",
        "colab": {}
      },
      "source": [
        "convert_bin([1,0,1,1,1], key=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'10111'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2n9Gh_kxYUo0",
        "colab_type": "code",
        "outputId": "df55c935-2d34-4725-bb71-e14df7f89104",
        "colab": {}
      },
      "source": [
        "convert_bin('10111', key=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 0, 1, 1, 1]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6HpxM2ubr57",
        "colab_type": "text"
      },
      "source": [
        "****Filter Library****"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9FQleL4YUo5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwjwEejTzd6T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "#list = array (indexnya 0,1,2, dst) tdk menentukan indexnya di awal, dict = hashmap ('a') harus mengisi key nya\n",
        "def data_separate(y, complement=False):\n",
        "    kelas = sorted(set(y))\n",
        "    dict_index = dict()\n",
        "    for c in kelas:\n",
        "        index = list()\n",
        "        for ix, c_ in enumerate(y):\n",
        "            if complement==False and c==c_:\n",
        "                index.append(ix)\n",
        "            elif complement==True and c!=c_:\n",
        "                index.append(ix)\n",
        "        dict_index.update({c:index})\n",
        "    return dict_index\n",
        "\n",
        "class ChiSquare:\n",
        "    def __init__(self, alpha = 0.001):\n",
        "        self.alpha = alpha\n",
        "\n",
        "        self.nilai_kritis = {\n",
        "            0.1   : 2.71,\n",
        "            0.05 : 3.84,\n",
        "            0.01 : 6.63,\n",
        "            0.005 : 7.88,\n",
        "            0.001 : 10.83\n",
        "        }\n",
        "\n",
        "        if self.alpha not in self.nilai_kritis:\n",
        "            print(self.alpha, \"tidak termasuk dalam tabel nilai kritis\")\n",
        "\n",
        "    def find_best_features(self, data, label, fitur = None):\n",
        "\n",
        "        if fitur == None:\n",
        "            vectorizer = CountVectorizer(binary=True)\n",
        "        else:\n",
        "            vectorizer = CountVectorizer(vocabulary = fitur, binary=True)\n",
        "\n",
        "        X = vectorizer.fit_transform(data)\n",
        "        nchi = X.shape[0]\n",
        "        self.fitur = vectorizer.get_feature_names()\n",
        "        index_doc = data_separate(label)\n",
        "        index_doc_complement = data_separate(label, complement=True)\n",
        "\n",
        "        self.classes = list(set(label))\n",
        "\n",
        "        self.dict_chis = {\"fitur\":self.fitur}\n",
        "        #fitur adalah key, self.fitur (isinya) berasal dr ln 44 mengambil semua fitur\n",
        "        # for c_ in list(reversed(self.classes)):\n",
        "        for c_ in self.classes:\n",
        "            A = np.sum(X[index_doc[c_]], axis=0).A\n",
        "            B = X[index_doc[c_]].shape[0] - A\n",
        "\n",
        "            C = np.sum(X[index_doc_complement[c_]], axis=0).A\n",
        "            D = X[index_doc_complement[c_]].shape[0] - C\n",
        "\n",
        "            AD = A*D\n",
        "            CB = C*B\n",
        "\n",
        "            atas = nchi * (AD - CB)**2\n",
        "            bawah = (A + C) * (B + D) * (A + B) * (C + D)\n",
        "            x_pangkat2 = atas/bawah\n",
        "            # print(atas, bawah)\n",
        "            self.dict_chis.update({c_:list(x_pangkat2[0])})\n",
        "            #memasukkan hasil perhitungan untuk setiap kelas disimpan dalam dict dimana key nya adalah kelasnya (c_)\n",
        "            #kalo di pyton berbasis objec, object akan bicara memory\n",
        "            # print(A, B, C, D, \"| \")\n",
        "            # print(\"=================================================\")\n",
        "        self.best_features = list()\n",
        "        nk = self.nilai_kritis[self.alpha] #Ini adalah implemenasi dari dictionary\n",
        "        # print(self.dict_chis[self.classes[0]])\n",
        "        for term, sp, nsp in zip(self.fitur, self.dict_chis[self.classes[0]], self.dict_chis[self.classes[1]]):\n",
        "            #akan dicek satu per satu fitur\n",
        "            #zip --> menyatukan beberapa list atau array, classes[0] adalah spam -> mengambil data chisquare utk kelas spam, classes[1] adalah nonspam\n",
        "            # print(sp, nsp)\n",
        "            if sp>=nk or nsp>=nk:\n",
        "                self.best_features.append(term)\n",
        "\n",
        "        self.data = pd.DataFrame.from_dict(self.dict_chis)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}