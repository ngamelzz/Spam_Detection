{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CzjTelUVyFkv",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Del3hgyTNEk5"
      },
      "source": [
        "**Sebelum Lanjut!,   jalankan \"Librari Wrapper-Forward\" di bagian paling bawah**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mU9Zy447yFk3",
        "outputId": "902e6904-9fb7-4e2e-c2e2-160aa410a73a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#data_cleanU1.xlsx\n",
        "data = pd.read_excel(\"https://github.com/lufias69/Spam_Detection/blob/master/Filter%20chi%20square/komentar/data_cleanU1.xlsx?raw=true\")\n",
        "# data = data[:200]\n",
        "komentar = data.komentar.tolist()\n",
        "label = data.label.tolist()\n",
        " \n",
        "y = np.array(label)\n",
        "print(len(y))\n",
        " \n",
        "data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5047\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Column1</th>\n",
              "      <th>label</th>\n",
              "      <th>komentar</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>195</td>\n",
              "      <td>spam</td>\n",
              "      <td>ga nyesel deh aku order sana</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>480</td>\n",
              "      <td>non spam</td>\n",
              "      <td>wakakak tolol tulang tulang</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>539</td>\n",
              "      <td>non spam</td>\n",
              "      <td>sepatu keren kamu keren cara instan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>564</td>\n",
              "      <td>spam</td>\n",
              "      <td>via q udh blja shoppe barang bagus kecewa mantap</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>596</td>\n",
              "      <td>non spam</td>\n",
              "      <td>sir what if use again kamisenglish that s just...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5042</th>\n",
              "      <td>5049</td>\n",
              "      <td>spam</td>\n",
              "      <td>bosen outfit kalian aja yuk cek ig tambah kole...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5043</th>\n",
              "      <td>5050</td>\n",
              "      <td>spam</td>\n",
              "      <td>maantaap banggeet kaak eteaah akuu pakee gemuk...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5044</th>\n",
              "      <td>5052</td>\n",
              "      <td>spam</td>\n",
              "      <td>sllu ajar usaha tahan kualitas costumer neng s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5045</th>\n",
              "      <td>5053</td>\n",
              "      <td>spam</td>\n",
              "      <td>aasi ane o a uda nenain odu yan sana eui eecay...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5046</th>\n",
              "      <td>5054</td>\n",
              "      <td>spam</td>\n",
              "      <td>awal ny ga percaya aku coba nyata hasil cepet ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5047 rows Ã— 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Column1     label                                           komentar\n",
              "0         195      spam                       ga nyesel deh aku order sana\n",
              "1         480  non spam                        wakakak tolol tulang tulang\n",
              "2         539  non spam                sepatu keren kamu keren cara instan\n",
              "3         564      spam   via q udh blja shoppe barang bagus kecewa mantap\n",
              "4         596  non spam  sir what if use again kamisenglish that s just...\n",
              "...       ...       ...                                                ...\n",
              "5042     5049      spam  bosen outfit kalian aja yuk cek ig tambah kole...\n",
              "5043     5050      spam  maantaap banggeet kaak eteaah akuu pakee gemuk...\n",
              "5044     5052      spam  sllu ajar usaha tahan kualitas costumer neng s...\n",
              "5045     5053      spam  aasi ane o a uda nenain odu yan sana eui eecay...\n",
              "5046     5054      spam  awal ny ga percaya aku coba nyata hasil cepet ...\n",
              "\n",
              "[5047 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoPekKK7cPVf",
        "colab_type": "text"
      },
      "source": [
        "**seleksi fitur menggunakan filter**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHKN9dZzcKNB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " \n",
        "sf = ChiSquare(alpha = 0.1)\n",
        "#inisialisasi\n",
        "sf.find_best_features(komentar, label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x74HTd1hdzX3",
        "colab_type": "code",
        "outputId": "a3d66cde-b5cf-4921-80f4-d1d0d54e5411",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "print(\"jumlah fitur terbaik:\",len(sf.best_features),\"dari\",len(sf.fitur), \"fitur\")\n",
        "best_features = sf.best_features\n",
        "sf.data #untuk memunculkan semua data\n",
        "sf.alpha #untuk mengetahui alpha"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "jumlah fitur terbaik: 2749 dari 15136 fitur\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38cjhBVIc69C",
        "colab_type": "text"
      },
      "source": [
        "**Seleksi fitur Wrapper-forward**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "t9c7riZ-yFk_",
        "outputId": "1af93a56-3b62-48a6-d4ab-f0d3d77c87c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#ini lama bangets\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "hasil = wrapper(model=MultinomialNB(),vocabs = best_features,  weighting=TfidfVectorizer, corpus=komentar, y=y,n_splits=5, target=0.97)\n",
        " \n",
        "best_fitur = hasil[1]\n",
        "for i in best_fitur:\n",
        "    print(i)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2749\n",
            "$.^^.\n",
            "1| 0.5557758392291162\n",
            "2| 0.6316651129929644\n",
            "3| 0.6835725990834961\n",
            "4| 0.7053665525125357\n",
            "5| 0.7204286176883297\n",
            "6| 0.7339031881384372\n",
            "7| 0.7751152498797946\n",
            "8| 0.8044443572206577\n",
            "9| 0.8189080454130646\n",
            "10| 0.8319857912451305\n",
            "11| 0.8412988057973291\n",
            "12| 0.8504126230264255\n",
            "13| 0.8583379289366002\n",
            "14| 0.8650735460067315\n",
            "15| 0.8718103405979845\n",
            "16| 0.8771608003218558\n",
            "17| 0.8829067108891266\n",
            "18| 0.8880597395715786\n",
            "19| 0.8930145521985301\n",
            "20| 0.8965820486904985\n",
            "21| 0.899753309324986\n",
            "22| 0.9025263715667899\n",
            "23| 0.9051025915277355\n",
            "24| 0.9076786152351607\n",
            "25| 0.9102540501820251\n",
            "26| 0.9120379946815296\n",
            "27| 0.9140195664759737\n",
            "28| 0.9160007457633773\n",
            "29| 0.9177841015023207\n",
            "30| 0.9193694374392841\n",
            "31| 0.9209543808692068\n",
            "32| 0.9223415007506697\n",
            "33| 0.9237284243786125\n",
            "34| 0.9249171319510543\n",
            "35| 0.9265020753809772\n",
            "36| 0.9278893915159603\n",
            "37| 0.9290780990884023\n",
            "38| 0.9302668066608446\n",
            "39| 0.9312574944313063\n",
            "40| 0.9322477896947277\n",
            "41| 0.9330406539167297\n",
            "42| 0.9338335181387315\n",
            "43| 0.9346261861072133\n",
            "44| 0.9354190503292154\n",
            "45| 0.9362117182976968\n",
            "46| 0.9370041900126583\n",
            "47| 0.9377966617276197\n",
            "48| 0.9385891334425811\n",
            "49| 0.9393814089040221\n",
            "50| 0.9401736843654633\n",
            "51| 0.9407687250390054\n",
            "52| 0.9415611967539667\n",
            "53| 0.9423530797083671\n",
            "54| 0.9429475316213484\n",
            "55| 0.9435419835343296\n",
            "56| 0.9441364354473109\n",
            "57| 0.944730887360292\n",
            "58| 0.9453253392732732\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sNnA5SntyFlE",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jNoLjYjbyFlJ",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jmNI1P8syFlO",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "r8Rj-XSPyFlT",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8frkFsAUyFlf",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6YEp-P_nJ1ow"
      },
      "source": [
        "**Librari Wrapper-Forward**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAvinpiKx0F3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        " \n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        " \n",
        "from sklearn.model_selection import StratifiedKFold\n",
        " \n",
        "def convert_bin(data, key=False):\n",
        "    new_data = list()\n",
        "    if (type(data)==list or type(data)==str) and key==False:\n",
        "        if type(data)==str:\n",
        "            for d in data:\n",
        "                new_data.append(int(d))\n",
        "        else:\n",
        "            for d in data:\n",
        "                new_data.append(int(d))\n",
        "        return new_data\n",
        "    else:\n",
        "        for d in data:\n",
        "            new_data.append(str(d))\n",
        "        return \"\".join(new_data)\n",
        "            \n",
        "def join_bin(data1, data2):\n",
        "    new_data = list()\n",
        "    for a,b in zip(data1, data2):\n",
        "        if a==b:\n",
        "            new_data.append(a)\n",
        "        elif a!=b:\n",
        "            if b==1:\n",
        "                new_data.append(b)\n",
        "            elif a==1:\n",
        "                new_data.append(a)\n",
        "            else:\n",
        "                new_data.append(0)\n",
        "    return new_data\n",
        " \n",
        "def gen(x, urutan):\n",
        "    first_bin = [0 for i in range(0,x)]\n",
        "    first_bin[urutan]=1\n",
        "    return first_bin\n",
        " \n",
        "def generate_bins(n):\n",
        "    bin_all = list()\n",
        "    for i in range(0,n):\n",
        "        bin_gen = gen(n,i)\n",
        "        bin_all.append(bin_gen)\n",
        "    return bin_all\n",
        " \n",
        "def convert_to_index(data):\n",
        "    new_data = list()\n",
        "    for idx, i in enumerate(data):\n",
        "        if i!=0:\n",
        "            new_data.append(idx)\n",
        "    return new_data\n",
        " \n",
        " \n",
        " \n",
        "def wrapper(vocabs, model=None, weighting=TfidfVectorizer(), corpus=None, y=None,n_splits=5, target=0.97):\n",
        "    \n",
        "    y=np.array(y)\n",
        "    vectorizer = weighting(vocabulary = vocabs)\n",
        "    X = vectorizer.fit_transform(corpus)\n",
        "    fitur = np.array(vectorizer.get_feature_names())\n",
        "    panjang_fitur = len(fitur)\n",
        " \n",
        "    print(len(vectorizer.get_feature_names()))\n",
        "    \n",
        "    binari_pertama = generate_bins(panjang_fitur)\n",
        "    scor_binari_pertama = list()\n",
        " \n",
        "    skf = StratifiedKFold(n_splits=n_splits)\n",
        "    skf_split = skf.split(X, y)\n",
        " \n",
        " \n",
        "    print(\"$\",end=\".\")\n",
        "    for b in binari_pertama:\n",
        "        idx_bin = convert_to_index(b)\n",
        "        # print(fitur[idx_bin],end=\"\")\n",
        "        vectorizer = weighting(vocabulary=fitur[idx_bin])\n",
        "        X = vectorizer.fit_transform(corpus)\n",
        "        \n",
        "        skf = StratifiedKFold(n_splits=n_splits)\n",
        "        skor_list = list()\n",
        "        \n",
        "        for train_index, test_index in skf.split(X, y):\n",
        "            # print(X[train_index])\n",
        "            X_train, X_test = X[train_index], X[test_index]\n",
        "            y_train, y_test = y[train_index], y[test_index]\n",
        "            \n",
        "            model.fit(X_train, y_train)\n",
        "            skor_list.append(model.score(X_test,y_test))\n",
        "            \n",
        "        scor_binari_pertama.append(sum(skor_list)/len(skor_list))\n",
        "    print(\"^^\",end=\".\") \n",
        "    print(\"\")\n",
        "    max_index = scor_binari_pertama.index(max(scor_binari_pertama))\n",
        "    pilihan_pertama_bin = binari_pertama[max_index]\n",
        "    print(sum(pilihan_pertama_bin), end=\"| \")\n",
        "    print(max(scor_binari_pertama))\n",
        "    \n",
        "    all_bin = list()\n",
        "    all_skor = list()\n",
        " \n",
        "    best_fitur = list()\n",
        "    best_skor = list()\n",
        "    \n",
        "    for _ in range(panjang_fitur-1):\n",
        "        # print(_, end=\".\")\n",
        "        new_bin_ = list()\n",
        "        \n",
        "        for idx, i in enumerate(binari_pertama):\n",
        "          if convert_bin(i, key=True) != convert_bin(pilihan_pertama_bin, key=True):\n",
        "            # if idx !=pilihan_pertama_bin:\n",
        "            new_bin_.append(join_bin(pilihan_pertama_bin, i))\n",
        "                \n",
        "        # print(sum(new_bin_[0]))\n",
        "        \n",
        "        binari_pertama = list(new_bin_)\n",
        "        scor_binari_pertama = list()\n",
        "        for b in new_bin_:\n",
        "            idx_bin = convert_to_index(b)\n",
        "            vectorizer = weighting(vocabulary=fitur[idx_bin])\n",
        "            X = vectorizer.fit_transform(corpus)\n",
        " \n",
        "            skf = StratifiedKFold(n_splits=n_splits)\n",
        "            skor_list = list()\n",
        "            for train_index, test_index in skf.split(X, y):\n",
        "                X_train, X_test = X[train_index], X[test_index]\n",
        "                y_train, y_test = y[train_index], y[test_index]\n",
        " \n",
        "                model.fit(X_train, y_train)\n",
        "                skor_list.append(model.score(X_test,y_test))\n",
        " \n",
        "            scor_binari_pertama.append(sum(skor_list)/len(skor_list))\n",
        "        #\n",
        "        max_index = scor_binari_pertama.index(max(scor_binari_pertama))\n",
        "        pilihan_pertama_bin = new_bin_[max_index]\n",
        "        print(sum(pilihan_pertama_bin), end=\"| \")\n",
        " \n",
        "        print(max(scor_binari_pertama))\n",
        " \n",
        "        best_fitur.append(fitur[convert_to_index(pilihan_pertama_bin)])\n",
        "        best_skor.append(max(scor_binari_pertama))\n",
        "        \n",
        "        # all_bin += list(new_bin_)\n",
        "        # all_skor += list(scor_binari_pertama)\n",
        "        \n",
        "        if max(scor_binari_pertama) >= target:\n",
        "            return (max(scor_binari_pertama), fitur[convert_to_index(pilihan_pertama_bin)])\n",
        "        elif sum(pilihan_pertama_bin)>= panjang_fitur-1:\n",
        "          return (max(scor_binari_pertama), fitur[convert_to_index(pilihan_pertama_bin)])\n",
        "        \n",
        "    max_index = best_skor.index(max(best_skor))\n",
        "    # pilihan_pertama_bin = new_bin_[max_index]\n",
        "    best_of_dbest_fitur = best_fitur[max_index]\n",
        "    return (max(best_skor), best_of_dbest_fitur)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6HpxM2ubr57",
        "colab_type": "text"
      },
      "source": [
        "****Filter Library****"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwjwEejTzd6T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        " \n",
        "import pandas as pd\n",
        " \n",
        "#list = array (indexnya 0,1,2, dst) tdk menentukan indexnya di awal, dict = hashmap ('a') harus mengisi key nya\n",
        "def data_separate(y, complement=False):\n",
        "    kelas = sorted(set(y))\n",
        "    dict_index = dict()\n",
        "    for c in kelas:\n",
        "        index = list()\n",
        "        for ix, c_ in enumerate(y):\n",
        "            if complement==False and c==c_:\n",
        "                index.append(ix)\n",
        "            elif complement==True and c!=c_:\n",
        "                index.append(ix)\n",
        "        dict_index.update({c:index})\n",
        "    return dict_index\n",
        " \n",
        "class ChiSquare:\n",
        "    def __init__(self, alpha = 0.001):\n",
        "        self.alpha = alpha\n",
        " \n",
        "        self.nilai_kritis = {\n",
        "            0.1   : 2.71,\n",
        "            0.05 : 3.84,\n",
        "            0.01 : 6.63,\n",
        "            0.005 : 7.88,\n",
        "            0.001 : 10.83\n",
        "        }\n",
        " \n",
        "        if self.alpha not in self.nilai_kritis:\n",
        "            print(self.alpha, \"tidak termasuk dalam tabel nilai kritis\")\n",
        " \n",
        "    def find_best_features(self, data, label, fitur = None):\n",
        " \n",
        "        if fitur == None:\n",
        "            vectorizer = CountVectorizer(binary=True)\n",
        "        else:\n",
        "            vectorizer = CountVectorizer(vocabulary = fitur, binary=True)\n",
        " \n",
        "        X = vectorizer.fit_transform(data)\n",
        "        nchi = X.shape[0]\n",
        "        self.fitur = vectorizer.get_feature_names()\n",
        "        index_doc = data_separate(label)\n",
        "        index_doc_complement = data_separate(label, complement=True)\n",
        " \n",
        "        self.classes = list(set(label))\n",
        " \n",
        "        self.dict_chis = {\"fitur\":self.fitur}\n",
        "        #fitur adalah key, self.fitur (isinya) berasal dr ln 44 mengambil semua fitur\n",
        "        # for c_ in list(reversed(self.classes)):\n",
        "        for c_ in self.classes:\n",
        "            A = np.sum(X[index_doc[c_]], axis=0).A\n",
        "            B = X[index_doc[c_]].shape[0] - A\n",
        " \n",
        "            C = np.sum(X[index_doc_complement[c_]], axis=0).A\n",
        "            D = X[index_doc_complement[c_]].shape[0] - C\n",
        " \n",
        "            AD = A*D\n",
        "            CB = C*B\n",
        " \n",
        "            atas = nchi * (AD - CB)**2\n",
        "            bawah = (A + C) * (B + D) * (A + B) * (C + D)\n",
        "            x_pangkat2 = atas/bawah\n",
        "            # print(atas, bawah)\n",
        "            self.dict_chis.update({c_:list(x_pangkat2[0])})\n",
        "            #memasukkan hasil perhitungan untuk setiap kelas disimpan dalam dict dimana key nya adalah kelasnya (c_)\n",
        "            #kalo di pyton berbasis objec, object akan bicara memory\n",
        "            # print(A, B, C, D, \"| \")\n",
        "            # print(\"=================================================\")\n",
        "        self.best_features = list()\n",
        "        nk = self.nilai_kritis[self.alpha] #Ini adalah implemenasi dari dictionary\n",
        "        # print(self.dict_chis[self.classes[0]])\n",
        "        for term, sp, nsp in zip(self.fitur, self.dict_chis[self.classes[0]], self.dict_chis[self.classes[1]]):\n",
        "            #akan dicek satu per satu fitur\n",
        "            #zip --> menyatukan beberapa list atau array, classes[0] adalah spam -> mengambil data chisquare utk kelas spam, classes[1] adalah nonspam\n",
        "            # print(sp, nsp)\n",
        "            if sp>=nk or nsp>=nk:\n",
        "                self.best_features.append(term)\n",
        " \n",
        "        self.data = pd.DataFrame.from_dict(self.dict_chis)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}